\documentclass[pldi]{sigplanconf-pldi15}
% Requires temporary version of sigplanconf style file provided on
% PLDI'15 web site.

%\def\inlinecode{\code}
%\def\code{\lstinline[basicstyle=\ttfamily]}
\newcommand{\code}[1]{\mintinline{c}{#1}}

\usepackage{supertech}
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage[hyphens]{url}                  % format URLs
% Try \usepackage[hyphens]{url}
% or try this
%% \makeatletter
%% \g@addto@macro{\UrlBreaks}{\UrlOrds}
%% \makeatother

%\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI

\usepackage{minted}
%\usepackage{pslatex}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{dblfloatfix}

%\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
%\newcommand{\punt}[1]{}

%\usepackage{titling}             % Uncomment both to   
%\setlength{\droptitle}{-2em}     % change title position 

\begin{document}
\special{papersize=8.5in,11in}% Still need this even with letterpaper
\title{SuperMalloc: A Super Fast Multithreaded \texttt{malloc()} for 64-bit Machines}
%\author{Bradley C. Kuszmaul \hspace{0.2in} \texttt{bradley@mit.edu}}
\date{}
\maketitle
\begin{abstract}
SuperMalloc is an implementation of \texttt{malloc(3)} originally
designed for X86 Hardware Transactional Memory (HTM)\@.  It turns out
that the same design decisions also make it fast even without HTM\@.
We compared SuperMalloc to DLmalloc, JEmalloc, Hoard, and TBBmalloc,
measuring speed, scalability, speed variance, memory footprint, and
code size.  On every benchmark we tried, by every measure, SuperMalloc
performs at least as well, and sometimes significantly better, than the
others.  The fastest alternative to SuperMalloc is TBBmalloc (which is
about the same speed as SuperMalloc) but TBBmalloc suffers from huge
memory footprints on some workloads.  JEmalloc is about 3.5 times
slower than SuperMalloc on the malloc-test benchmark.  Hoard is half
as fast as JEmalloc. DLmalloc is an order of magnitude slower than
Hoard.)

SuperMalloc's footprint is small.  For some workloads, Hoard is about
30\% larger, and DLmalloc and JEmalloc are often twice as large.
TBBmalloc appears to have an unbounded footprint --- we often saw an
footprints an order of magnitude larger than SuperMalloc.  None of the
widely-used benchmarks consistently stressed the footprint created by
a multithreaded workload, so we created one.

SuperMalloc achieves these performance advantages using half as much
code as the alternatives.  SuperMalloc exploits the fact that although
physical memory is always precious, virtual address space on a 64-bit
machine is cheap to use simpler faster data structures.
\end{abstract}

\secput{intro}{Introduction}

C/C++ dynamic memory allocation functions (\code{malloc(3)} and
\code{free(3)}) can impact the cost of running applications.  The cost
can show up in several ways: allocation operations can be slow for
serial programs, they can fail to scale with the number of cores in a
multithreaded multicore environment, they can occasionally be slow,
and they can induce a large memory footprint.  Furthermore, if the
allocator itself is too complex, it can inhibit improvements.  We can
divide these problems roughly into three categories: speed, footprint,
and complexity. 

The rest of this section explores this space in the context of several
allocators.  The allocators include DLmalloc~\cite{Lea96},
Hoard~\cite{BergerMcBl00}, JEmalloc~\cite{Evans06},
TBBmalloc~\cite{KukanovVo07}, and our allocator, SuperMalloc.
\figref{codesize} shows the code size for the allocators, including
our allocator, SuperMalloc.

Many modern allocators take advantage of the fact that the operating
system allocates memory in two steps corresponding to virtual
allocation and physical allocation: First a system call such as
\code{mmap()} allocates a contiguous range of virtual addresses.  At
this point, no physical memory has been allocated.  Later, when the
application actually touches the virtual addresses, a page fault
occurs, and the operating system allocates physical memory.  A page of
virtual memory that has physical memory associated with it is called a
\defn{committed} page, whereas if no physical memory is allocated, the
page is \defn{uncommitted}.  An application or library can use
\code{madvise()} to uncommit a committed page, returning the
underlying physical memory back to the operating system.

{\paragraph{DLmalloc}} \cite{Lea96} is the default single-threaded
allocator in Linux.  DLmalloc is relatively simple and has been stable
for decades.  DLmalloc employs per-object boundary tags (which are due
to~\cite{Knuth73}).  On a 64-bit machine, each allocated object is
immediately preceded by 8 bytes which indicate the size of the object
(and hence the address of the next object, as well as whether the
object is allocated or free, and whether the previous object is
allocated or free.  When the previous object is free, the last 8 bytes
of that object also indicate the size.  When DLmalloc frees an object,
it can immediately coalesce the object with the next object (if it is
free) since it knows the size of the newly freed object.  If the
previous object is freed, it can also coalesce the newly freed object
with the previous one.  The boundary tags can contribute up to a 50\%
space overhead when allocating small (8-byte) objects.  DLmalloc
employs a first-fit heuristic, and has no provision for multithreading
performance beyond using a single lock to protect the entire
allocation data structure.  For example, DLmalloc has no per-thread
caching.

DLmalloc has been placed in the public domain.

{\paragraph{Hoard}} \cite{BergerMcBl00} was introduced during the era
of the first multithreaded allocators.  The original Cilk
allocator~\cite{BlumofeLe94} and the STL allocator of the
time~\cite{SGI97} provided per-thread allocation, which was very fast
(no locking was required), but also produced unbounded memory blowups.
Some allocators, such as PTmalloc~\cite{Gloger06} and
LKmalloc~\cite{LarsonKr98} exhibit memory blowup bounded by $P$, the
number of processors.  Hoard provides a provable bound on its space
blowup.  Hoard organizes small objects into \defn{chunks}, which
contain objects all the same size.\footnote{Chunks are called
  ``superblocks'' in \cite{BergerMcBl00}.  They are called ``chunks''
  in \cite{Evans06} and \cite{KukanovVo07}.}  (Large objects are dealt
with using heavier-weight mechanisms).  Hoard puts metadata at the
beginning of each superblock indicating the size of the objects in the
superblock.  Since the objects within a superblock are all the same,
Hoard does not need boundary tags.

Hoard uses an \defn{allocate-fullest} heuristic.  Hoard sometimes
finds itself in a situation where an object could be allocated out of
any one of several superblocks.  Hoard allocates the object out of the
fullest such superblock. By allocating into the fullest superblock,
Hoard improves the chances that a superblock will completely empty can
be reused for other purposes.

Hoard is licensed under GPLv2.

\begin{figure}
\begin{tabular}{lrr}
Allocator & Bytes        & LoC \\ \hline
 DLmalloc    \cite{Lea96}        & 221K & 28K \\
 Hoard       \cite{BergerMcBl00} & 429K & 59K \\
 JEmalloc    \cite{Evans06}      & 624K & 69K \\
 TBBmalloc   \cite{KukanovVo07}  & 350K & 36K \\
 SuperMalloc & 127K & 15K \\
\end{tabular}
\caption{Code sizes measured in bytes and in Lines of Code (LoC).
  Note: TBBmalloc is tough to measure since it's mixed into TBB.
  SuperMalloc could end up with more code as it becomes
  production-quality.}
\label{fig:codesize}
\end{figure}

{\paragraph{JEmalloc}} \cite{Evans06} is, in our experience, the best
allocator now available for production.  JEmalloc ships with BSD and
Mozilla, as well as with MariaDB in recent Red Hat distributions.
JEmalloc strives to minimize footprint in long-running processes such
as browsers and database servers.  Like Hoard, JEmalloc allocates
large chunks of memory containing objects all the same size.  

Whereas Hoard uses an allocate-fullest heuristic, JEmalloc uses a
lowest-address heuristic: Of all the possible objects to allocate,
JEmalloc allocates the one with the lowest address.  This strategy is
something akin to a first-fit strategy, except that all the objects
are the same size.  First fit for fixed-size objects is generally
pretty good at freeing up entire blocks, but it seems likely that
fullest-fit is generally better.

JEmalloc uses a thread-local cache, and eventually returns objects
from the thread-local cache globally accessible memory.  JEmalloc is
faster than Hoard, and although JEmalloc does not provide any explicit
bounds on memory blowup, it seems to be the allocator of choice for
long-running multithreaded processes that need to keep their footprint
under control.

JEmalloc uses three size categories: small, large, and huge, and
implements them differently. For small and large objects, JEmalloc
carves a chunk into page runs using a buddy algorithm, and maintains
metadata at the beginning of the chunk.  By storing metadata at the
beginning of the chunk, the pages themselves can remain untouched, and
uncommitted.

JEmalloc directly maps huge objects \code{mmap()}.  Huge objects are
chunk-aligned.  JEmalloc maintains a red-black tree mapping huge chunk
addresses to metadata.

JEmalloc is licensed under a variant of the modified BSD license.

{\paragraph{TBBmalloc}} \cite{KukanovVo07} is the allocator shipped
with Intel's Threading Building Blocks (TBB)\@.  TBBmalloc is about as
fast as SuperMalloc, but has an unbounded space blowup in theory and
in practice.  TBB use a thread-private heap, and never returns space
for small objects to the operating system.  TBBmalloc always allocates
out of a per-thread pool with no locking.  If the same thread returns
the object as allocated it, then no locking is needed, otherwise the
object is being returned to a \defn{foreign} block, and requires
lightweight synchronization to place the object into a linked list
that will be dealt with the next time the owner of the foreign block
tries to allocate.  Like JEmalloc and Hoard, TBBmalloc uses chunks
each containing objects of homogeneous size, and places metadata at
the beginning of each chunk.

TBBmalloc is licensed under GPLv2.

\secput{supermalloc}{SuperMalloc}

\secref{intro} reviewed the competition, and this section explains how
SuperMalloc works.   

Like the other chunk-oriented allocators (Hoard, JEmalloc, and
TBBmalloc), SuperMalloc allocates large chunks of homogeneous-sized
objects for small objects, and uses operating-system-provided memory
mapping for larger objects.  SuperMalloc's chunks are multiples of
$2$~MiB, and are $2$~MiB-aligned, which corresponds to huge pages on
x86.  In operating systems that support huge-pages, chunks align
properly with the paging system.  For objects that are significantly
less than $1$~MiB, SuperMalloc carves chunks into small pieces.  For
larger objects, the allocation is backed by one or more contiguous
chunks.

Unlike the other chunk-oriented allocators, SuperMalloc does not
divide its chunks into smaller pieces.  For example, JEmalloc divides
its chunks into runs using a buddy algorithm, and each run can contain
objects of a different size.  In SuperMalloc, the entire chunk is the
same size.  The rationale for this decision is that on a 64-bit
machine, virtual-address space is relatively cheap, while physical
memory remains dear.  The other allocators' approach of dividing up a
chunk saves virtual-address space by requiring fewer chunks to be
allocated.  In the case where that strategy actually does save space,
it is because the application did not actually allocate a whole
chunk's worth of objects of a given size.  In that case, in
SuperMalloc, the rest of the block is an untouched uncommitted
``wilderness area'' (a term apparently coined by \cite{KornVo85})
using no memory.

Like JEmalloc, SuperMalloc employs several object sizes.  The sizes
are organized as into bins.  In SuperMalloc, there are small, medium,
and huge objects.  The first 45 bins are specific sizes, and larger
bin numbers encode the number of pages that the application actually
allocated (so that when we map a 2~MiB chunk to support a 1.5~MiB
request, we can properly track the amount of physical memory that the
application has asked for).  A 4-byte bin number allows us to allocate
objects of up to just under $2^{32}$ pages ($2^{44}$ bytes).

Given a pointer to an object, a chunk-based allocator must be able to
determine which chunk an object belongs to, and what the size of the
objects in that chunk are.  Here, for comparison, we focus on the
comparison with JEmalloc for which the internal data structures are
explained in some detail.  In JEmalloc, depending on the size of the
object, the object might be looked up in a global red-black tree (for
huge objects), or in a buddy-allocation data structure at the
beginning of a chunk.  

SuperMalloc adopted a simpler strategy.  An entire chunk is all the
same sized, and we can look up the chunk number in a table.  Instead
of using a tree, SuperMalloc uses an array to implement that table,
however.  Since the usable x86 address space is $2^{48}$ bytes and
chunks are $2^{21}$ bytes, there are only $2^{27}$ entries in the
table.  Each entry is a 4-byte bin number.  The SuperMalloc table
consumes $512$MiB of virtual address space, but since the operating
system employs a lazy commit strategy, we typically need only a few of
pages physical memory for the chunk table.  This is another instance
of the design principle for 64-bit software that ``it is OK to waste
some virtual address space.''

{\paragraph{Small objects:}} Small objects are as small as 8 bytes,
and increase in size by at most 25\% to limit internal fragmentation.
Small objects are regularly spaced: the sizes are $8, 10, 12, 14, 16,
20, 24, \ldots, 224$, and their sizes take the form $k\cdot2^i$ for
$4\leq k \leq 7$ and $1\leq i \leq 5$.  In other words, a small object
size, when written in binary is a $1$, followed by two arbitrary
digits, followed by zeros.  Thus bin~$0$ contains $8$-byte objects,
bin~1 contains $10$-byte objects, and so forth.  To compute the bin
number from a small size can be done with bit hacking in $O(1)$
operations using the code shown in \figref{size2bin}.

\begin{figure}
\begin{minted}[mathescape]{c}
int size_2_bin(size_t s) {
  if (s <= 8) return 0;
  if (s <= 320) {
    // Number of leading zeros in $s$.
    int z = clz(s);
    // Round up to the relevant
    // power of $2$.
    size_t r = s + (1ul<<(61-z)) -1;
    int y = clz(r);
    // $y$ indicates which power of two.
    // $r$ shifted and masked indicates
    //  what the low-order bits are.
    return 4*(60-y)+ ((r>>(61-y))&3);
  }
  if (s <= 448) return 22;
  if (s <= 512) return 23;
  ..
  if (size <= 1044480) return 45;
  return 45 + ceil(size-1044480, 4096);
}
\end{minted}
\caption{Code to convert a size to a bin number.  The first 22 bins
  are handled by bit hacking. The \code{clz()} function returns the
  number of leading 0 bits in its argument. Bins~22--45 are handled by
  a case statement (the elided sizes are $704$, $832$, $1024$, $1088$,
  $1472$, $1984$, $2048$, $2752$, $3904$, $4096$, $5312$, $7232$,
  $8192$, $10048$, $14272$, $16384, 32768$, $65536$, $131072$,
  $258048$, and $520192$.)  Huge bins are computed as a constant plus
  the number of pages used by the object.}
\label{fig:size2bin}
\end{figure}

For small objects, we make no particular effort to avoid false
sharing.  Like JEmalloc, we assume that users who care about false
sharing will explicitly ask for cache-line alignment, using e.g.,
\code{posix_memalign()}.  This decision stands in contrast to
allocators, such as Hoard, that try to use temporal locality to induce
spatial locality: Hoard has heuristics that try to arrange to place
objects that were allocated at the same time by the same thread onto
the same cache line, and objects that were allocated by different
threads on different cache lines.  As explained below, we take a
different tack on avoiding false sharing, focusing on objects that are
larger than a cache line.

We reserve the first few pages of each 2~MiB chunk for bookkeeping.
The bookkeeping comprises a bitmap of the free objects and a heap-like
data structure for implementing the fullest-fit
heuristic. \cnote{red}{Explain how we implement fullest fit}{bck}


{\paragraph{Medium objects:}} Medium objects are all sized be a
multiple of 64 (the cache line size).  The smallest is 256 bytes, and
the largest is 14272 bytes.  The sizes we chose include powers of two
and prime multiples of cache lines.  The powers of two ($256$, $512$,
$1024$, $2048$, $4096$, $8192$) are used only when the application
explicitly asks for aligned data.  The prime multiples of cache lines
are used for all other requests to reduce the contention in the
low-associativity processor caches. For example, recent Intel
processors provide 8-way associative level~1 cache.  Following the
lead of \cite{AfekDiMo11}, which propose \defn{punctuated arrays} to
reduce associativity conflicts, we wanted to avoid aligning objects
into the same cache associativity set. (Apparently punctuated arrays
are appearing in most of the Solaris~12 allocators \cite{Dice14b}.)
We, however, took the more ``radical'' approach mentioned by
\cite{Dice14a} of making all the size classes a prime multiple of the
cache line, which not only reduces conflicts within a class size but
reduces inter-size cache-index conflicts.

One aspect that becomes relatively more important for medium objects
than small objects is what happens when the page size is not a
multiple of the object size.  Consider for example the 832-byte size
bin (832 bytes corresponds to 13 cache lines).  If we fill up a
4096-byte page with these objects, we would be able to fit 4 objects
into a page wasting 768 bytes at the end.  

Supermalloc avoids this internal fragmentation by using larger logical
pages, called \defn{folios} for allocation bookkeeping.  For example,
in the 832-byte bin, rather than wasting those 768 bytes, we allocate
objects out of a folio containing 13 pages (53,248 bytes). When we
allocate an 832-byte object, we find the fullest 13-page folio (among
all 832-byte folios in the system) that has a free slot, and allocate
that slot.  When a folio becomes empty, we uncommit the entire folio,
rather than trying to uncommit individual pages of a folio.  There is
still fragmentation at the end of the 2~MiB chunk (5 unused pages in
this case), but that fragmentation comprises whole pages which are
never touched and remain uncommited.  Once again, we do not mind
wasting a little virtual address space.

Except for the way their sizes are chosen, medium and small objects
are managed exactly the same way (with folios and the fullest-fit
heuristic, for example) by exactly the same code.

{\paragraph{Large objects:}} Large objects start at 16KiB (4 pages)
and go up to half a chunk size.  Large objects are all a multiple of a
page size, and the code is a little bit simpler than for small and
medium objects, since it mostly does not matter which large object is
returned by \code{\malloc()}; When a large object is freed its pages
can be uncommitted, so we don't need to keep track of anything for a fullest-fit heuristic.

In order to avoid associativity issues, we add random number of cache
lines, up to a page's worth, to the allocation, and adjust the
returned pointer by that random number.  For example when asked to
allocate $20,000$ bytes we pick a random number from 0 to 63 and add
that many cache lines to the request.  If, for example, we picked 31
for the random number, we would allocate $22,048 = 20,000+31\cdot64$
bytes.  That would be allocated out of bin~40, which contains $32$KiB
objects.  Instead of returning a pointer to the beginning of the
$32$KiB object, we return the pointer plus $2048$ bytes.  This
strategy wastes at most an extra page per object, and since we use
this strategy only on objects that are $4$ pages or larger, it can
result in at most $25$\% internal fragmentation (which matches the
fragmentation we were willing to tolerate for the small objects).  If
the application explicitly asks for a page-aligned allocation, we skip
the random rigamarole.  Adding a random-offset to what would otherwise
be a page-aligned allocation appeared in \cite{LvinNoBe08} for a
page-per-object allocator, and is applied in a more systematic fashion
by \cite{AfekDiMo11}.

This approach introduces one big gap between $7$~and $11$~cache lines
introducing internal fragmentation of up to 57\% for an allocation of
449 bytes, and slightly smaller gap between 13 cache lines and 17
cache lines ($31$\% fragmentation for 833-byte allocations).  To close
these gap, we plan to add two more bins at 9 cache lines and 15 cache
lines, with the rationale that a little bit of inter-size
associativity conflict is better than internal fragmentation.  (We
haven't yet made this change, but it should not affect any of the
performance results shown below.)

{\paragraph{Arithmetic:}} One common operation in SuperMalloc is to
calculate indexes in a bitmap.  For example, given a pointer $p$, we
must perform calculation in \figref{bitindex}(a) to calculate which folio
and which object in the folio $p$ refers to.

\begin{figure*}
\begin{minted}[mathescape,linenos,xleftmargin=1cm]{c}
C = p / chunksize;          // Compute chunk number.
B = chunkinfos[C];          // What is the bin?
FS = foliosizes[B];         // What is the folio size?
FN = (p%chunksize)/FS;      // Which folio in the chunk?$\label{li:FN}$
OS = objectsizes[B];        // How big are the objects?
ON = ((p%chunksize)%FS)/OS; // Which object number in the folio?$\label{li:ON}$
\end{minted}
\begin{center}
(a)
\end{center}
\begin{minted}[mathescape,linenos,xleftmargin=1cm]{c}
C = p / chunksize;
B = chunkinfos[C];
FS = foliosizes[B];
FN = ((p%chunksize)*magic0[B]) >> magic1[B];       // magic
OS = objectsizes[B];
ON = ((p%chunksize-FN*FS)*magic2[B]) >> magic3[B]; // magic
\end{minted}
\begin{center}
(b)
\end{center}
\caption{The calculation to compute the folio number in the chunk,
  \code{FN}, and the object number in the folio \code{ON}, so that the
  bitmap for the free objects in the folio can be updated.  (a) shows
  the code with expensive divisions in \lireftwo{FN}{ON}.  (b) shows
  the code with the divisions replaced by multiplication and shift.}
\label{fig:bitindex}
\end{figure*}


Since the chunk size is a power of two, computing the chunk number is
easy.  Computing the folio number requires dividing by the folio size,
which isn't a power of two, and isn't known at compile time, however.
Division is slow, so we follow the approach of
\cite{MagenheimerPePe87} to convert division to multiplication and
shift.  For example, for 32-bit values of \code{c}, we have
\mintinline{c}{x / 320 == (x*6871947674lu)>>41}.  \figref{bitindex}(b)
shows the faster code.  We use metaprogramming to generate all the
object sizes (e.g., to find prime numbers that are spaced no more than
25\% apart, and to calculate the multiplication and shift constants to
implement division.).


\paragraph{Thread and Processor Caching:}

A second innovation is to employ a per-CPU cache, in addition to a
per-thread cache.  The cost of locking and accessing a shared data
structure turns out to be mostly due to cache coherence traffic from
accessing the object, rather than the locking instructions.  E.g., on
our 2-socket machine, locking and accessing a contended object costs
$460$ns.  Locking and accessing an uncontended object (one that was
last accessed on the same processor) costs only 26ns.  Accessing a
thread-local object with no locking costs $3$ns.  The SuperMalloc
per-thread cache contains just enough objects to amortize the $23$ns
uncontended locking overhead.  The per-CPU cache contains just enough
objects in each class to fill up the hardware cache, the rationale
being that if the working set of a thread is bigger than the hardware
cache, there is no point in working hard to avoid a few more cache
misses.  In contrast, the other allocators may have many megabytes in
their per-thread caches (and because there are many per-thread caches,
their footprint is larger.)  Thus, a second design principle is
``uncontended locking is not so bad''.



--------------

The others allocate large aligned chunks which contain objects that
are all the same size, and provide per-thread caches.


{\paragraph{Speed:}} A poor allocator can be slow even on serial
applications, and the situation can get far worse on multithreaded
codes where the allocator can become a serial bottleneck.  On a
multicore with a multithreaded workload the speed difference between
the default allocator in Linux \cite{Lea96} and a state-of-the-art
allocator such as JEmalloc~\cite{Evans06} can be more than a factor of
30.  If the cost of allocation varies significantly it can be
difficult to meet the soft real-time constraints of a server.  We have
seen reports of \code{malloc()} causing a 3~second pause about once
per day on a database server, which would be too slow for a
social-networking site.  It was this 3~second pause that led us to
investigate how to achieve the performance of JEmalloc with less
variance.

{\paragraph{Footprint:}} The \defn{memory footprint} of an
application, the amount of physical memory that the application
consumes while running, can also vary by more than an order of
magnitude --- even a factor of two can be too much on something like a
database server where memory is used as a cache for disk, and an
increased footprint results in either a reduced effective cache size
or excessive I/Os due to paging.  Both JEmalloc~\cite{Evans06} and Hoard~\cite{BergerMcBl00} 

{\paragraph{Complexity:}} A simple memory allocator can operate using
only a few hundred lines of code \cite{KernighanRi88}.  Since
allocator performance is so complex, however, most allocators have
been tuned to run faster at the cost of increased complexity.  



only Even relatively simple allocators such as the

SuperMalloc is an implementation of \texttt{malloc(3)} originally designed for
x86 Hardware Transactional Memory (HTM).  
It turns out that the same
design also makes it fast even without HTM.  

On high-threadcount
workloads on Sandy Bridge (non-HTM) hardware, SuperMalloc outperforms
the competition by about a factor of 3.5.

% DLmalloc:    221K chars, 28K lines
% Hoard:       429K chars, 59K lines
% JEmalloc:    624K chars, 69K lines
% Tbbmalloc:   350K chars, 36K lines (tough to measure since it's mixed into TBB)
% SuperMalloc: 127K chars, 15K lines



* Tradeoff: Code size and complexity, speed, footprint, variability.  Supermalloc appears to disprove this tradeoff.
* Allocators use trees, buddy systems: supermalloc doesn't.
* Work for HTM and locks.


Notes: 
\begin{itemize}
\item \cite{ReinefeldDoSc13} shows the importance of malloc.
\item \cite{Dementiev09} shows what?
\item \cite{KukanovVo07} their cross-test sounds similar to malloc-test
\item \cite{Vyukov08} 
\item \cite{DetlefsDoZo94} Benchmarks include gs running on a 126-page manual.
\item \cite{HudsonSaAd06}
\item \cite{Michael04} shows a lock-free mallocator based on Hoard, with some interesting benchmarks: Linux-scalability, threadtest, larson and a producer-consumer test.
\item \cite{FengBe05}
\item \cite{SchneiderAnNi06}: streamflow (worries about cache behavior by using segregated heaps).  Synchronization-free operations for local alloc/free, and non-blocking synch for remote deallocations.  [In contrast, SuperMalloc doesn't care as much about trying to get cache behavior to be local.  Perhaps wrongly so.  Supermalloc does worry about false sharing  ].  Does poorly on memory footprint on the Larson benchmark, but they didn't isolate the issue, since they ran Larson for a fixed time instad of a fixed amount of work.
\item \cite{LarsonKr98} LKmalloc hashes the threadid (so it doesn't need so many arenas, but it still needs a lock.)  They say for servers the requirements are to be fast, high-memory utilization, size indenpendent, locality, scalable, thread independence, predictable speed, stability).  LKmalloc is similar DLmalloc in that it uses bins of free blocks, approx best fit, and immediate coaslescing.  LKmalloc improves on DLmalloc by providing a lock per free list, multiple subheaps (to reduce cache sloshing (a value migrating around), select subheap by hasing, and striping (4MB pieces allocated to a heap)
\item \cite{Gloger12} presents measurements of several allocators, with an emphasis on ptmalloc.
\item \cite{Gloger06} is the web page for ptmalloc, an allocator based on dlmalloc \cite{Lea96}.
\item Dice blog
\item knary a benchmark we should use (since I helped write
  it...).  Knary was used in \cite{SchneiderAnNi06}, but was
  attributed to Hood when in fact it's due to \cite{BlumofeJoKu96}.
\end{itemize}


{\bf Background:}   

{\bf Performance:} \figref{data} compares the performance of
SuperMalloc against DLmalloc, Hoard, and JEmalloc on the malloc-test
benchmark~\cite{LeverBo00}.  The malloc-test benchmark runs $K$
producer threads and $K$ consumer threads.  Each producer thread
allocates objects as fast as possible, and each consumer frees them.
Malloc-test offers one of the most difficult workloads for
multithreaded allocators that employ per-thread caching, since the
per-thread caches can quickly become unbalanced.


{\bf How they work:} 



\begin{figure}
\input{new-malloc-test-1K-tempo-aggregated}
\caption{A comparison of SuperMalloc, DLmalloc \cite{Lea96}, Hoard
  \cite{BergerMcBl00}, and JEmalloc~\cite{Evans06} running malloc-test
  on a 16-core (2 sockets + hyperthreading) 2.4GHz E5-2665 (Sandy
  Bridge).  The lines are the average of 8 trials, and the error bars
  show the fastest and slow trial.}
\label{fig:data}
\vspace*{-3ex}
\end{figure}

%SuperMalloc uses simple data structures to try to maximize the odds that hardware transactions succeed. For example, the SuperMalloc uses a priority heap to allocate objects out of the fullest page. The heap takes advantage of the fact that for each class, there are only a relatively small number of possible page-fullness values. For example, for 8-byte objects, there are only 512 objects in a page, and so instead of using a general heap, SuperMalloc uses an array of 513 lists, the $i$th list containing a list of pages with $i$ free slots.



%SuperMalloc uses a per-CPU cache and a per-thread cache. A per-thread cache of objects reduces the overhead of allocating and freeing objects because some objects do not need to perform any mutual exclusion. It turns out that most of the cost of updating a global data structure is due to cache misses, not the locking itself, however. On Haswell, an unlocked update to an uncontended cache line costs 3--5ns, whereas acquiring a lock and modifying uncontended variable costs 80ns. Acquiring an uncontended lock and updating an uncontended page costs only about 18ns. (On a multisocket Sandy Bridge processor, the difference is even more striking: 3ns uncontended unlocked, 26ns uncontended locked, and 460ns contended locked.) Accordingly, SuperMalloc uses a relatively small per-thread cache (containing only a few objects of each size), and uses a per-CPU cache (a cache per hardware thread). The per-CPU cache contains only one L3-cache worth of objects for each size class, since we assume that the application will actually store data into allocated objects. If the objects in the per-CPU cache don't fit in the L3 cache, then filling the objects will cause cache misses anyway, so it does not matter whether allocating the objects causes a few cache misses.

% To further improve the odds of transactions committing, SuperMalloc tries to prefetch into cache all the data of the transaction. Prefetching data appears to improve performance by about 5%.

%SuperMalloc appears to enjoy a substantially smaller footprint than the other allocators for two reasons. (1) SuperMalloc adopts Hoard's allocate-in-fullest-page heuristic rather than JEmalloc's approach of allocating the object with the lowest address. (2) SuperMalloc's caches are smaller than Hoard's or JEmallocs, mostly because the per-thread cache is extremely small, and in many applications there are far more threads than there are CPU's.

% SuperMalloc is currently implemented, and we are assembling and running the allocation benchmarks mentioned in other allocation papers. We plan to release the software under the Apache 2.0 license, and the assembled benchmarks under an appropriate mix of licenses.

SuperMalloc, like Hoard, allocates objects out of the fullest possible page in order to reduce memory footprint.

madv\_dontneed (and other wishlists from the talk)

I will soon release SuperMalloc under an open-source license.

{\raggedright
\bibliographystyle{abbrvnat}
\bibliography{allpapers}
}

%% \begin{figure*}
%% \begin{tabular}{cc}
%% \input{new-malloc-test-1K-tempo-aggregated}
%% &
%% \input{new-malloc-test-1K-lutestring-aggregated}
%% \end{tabular}
%% \caption{A comparison of SuperMalloc, dlmalloc, Hoard, and jemalloc on the malloc-test benchmark.}
%% \label{fig:data}
%% \end{figure*}

\end{document}

%%  LocalWords:  SuperMalloc Transactional threadcount multithreaded
%%  LocalWords:  allocator allocators JEmalloc HTM DLmalloc malloc ns
%%  LocalWords:  uncontended superblocks TBB TBBmalloc GPLv LKmalloc
%%  LocalWords:  superblock PTmalloc Cilk multithreading multicore
%%  LocalWords:  metadata unallocated uncommit madvise scalability
%%  LocalWords:  SuperMalloc's codesize mmap STL supermalloc
%%  LocalWords:  uncommited associativity rigamarole metaprogramming
