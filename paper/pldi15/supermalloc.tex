\documentclass[pldi]{sigplanconf-pldi15}
% Requires temporary version of sigplanconf style file provided on
% PLDI'15 web site.

%\def\inlinecode{\code}
%\def\code{\lstinline[basicstyle=\ttfamily]}
\newcommand{\code}[1]{\mintinline{c}{#1}}

\usepackage{supertech}
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage[hyphens]{url}                  % format URLs

\ifnotes
\newcommand{\bcknote}[1]{\cnote{red}{#1}{bck}}
\else
\fi

% Try \usepackage[hyphens]{url}
% or try this
%% \makeatletter
%% \g@addto@macro{\UrlBreaks}{\UrlOrds}
%% \makeatother

%\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI

\usepackage{minted}
%\usepackage{pslatex}
\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{dblfloatfix}

%\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
%\newcommand{\punt}[1]{}

%\usepackage{titling}             % Uncomment both to   
%\setlength{\droptitle}{-2em}     % change title position 

\begin{document}
\special{papersize=8.5in,11in}% Still need this even with letterpaper
\title{SuperMalloc: A Super Fast Multithreaded \texttt{malloc()} for 64-bit Machines}
%\author{Bradley C. Kuszmaul \hspace{0.2in} \texttt{bradley@mit.edu}}
\date{}
\maketitle
\begin{abstract}
SuperMalloc is an implementation of \texttt{malloc(3)} originally
designed for X86 Hardware Transactional Memory (HTM)\@.  It turns out
that the same design decisions also make it fast even without HTM\@.
We compared SuperMalloc to DLmalloc, JEmalloc, Hoard, and TBBmalloc,
measuring speed, scalability, speed variance, memory footprint, and
code size.  On every benchmark we tried, by every measure, SuperMalloc
performs at least as well, and sometimes significantly better, than the
others.  The fastest alternative to SuperMalloc is TBBmalloc (which is
about the same speed as SuperMalloc) but TBBmalloc suffers from huge
memory footprints on some workloads.  JEmalloc is about 3.5 times
slower than SuperMalloc on the malloc-test benchmark.  Hoard is half
as fast as JEmalloc. DLmalloc is an order of magnitude slower than
Hoard.)

SuperMalloc's footprint is small.  For some workloads, Hoard is about
30\% larger, and DLmalloc and JEmalloc are often twice as large.
TBBmalloc appears to have an unbounded footprint --- we often saw an
footprints an order of magnitude larger than SuperMalloc.  None of the
widely-used benchmarks consistently stressed the footprint created by
a multithreaded workload, so we created one.

SuperMalloc achieves these performance advantages using half as much
code as the alternatives.  SuperMalloc exploits the fact that although
physical memory is always precious, virtual address space on a 64-bit
machine is cheap to use simpler faster data structures.
\end{abstract}

\secput{intro}{Introduction}

C/C++ dynamic memory allocation functions (\code{malloc(3)} and
\code{free(3)}) can impact the cost of running applications.  The cost
can show up in several ways: allocation operations can be slow for
serial programs, they can fail to scale with the number of cores in a
multithreaded multicore environment, they can occasionally be slow,
and they can induce a large memory footprint.  Furthermore, if the
allocator itself is too complex, it can inhibit improvements.  We can
divide these problems roughly into three categories: speed, footprint,
and complexity. 

The rest of this section explores this space in the context of several
allocators.  The allocators include DLmalloc~\cite{Lea96},
Hoard~\cite{BergerMcBl00}, JEmalloc~\cite{Evans06},
TBBmalloc~\cite{KukanovVo07}, and our allocator, SuperMalloc.
\figref{codesize} shows the code size for the allocators, including
our allocator, SuperMalloc.

Many modern allocators take advantage of the fact that the operating
system allocates memory in two steps corresponding to virtual
allocation and physical allocation: First a system call such as
\code{mmap()} allocates a contiguous range of virtual addresses.  At
this point, no physical memory has been allocated.  Later, when the
application actually touches the virtual addresses, a page fault
occurs, and the operating system allocates physical memory.  A page of
virtual memory that has physical memory associated with it is called a
\defn{committed} page, whereas if no physical memory is allocated, the
page is \defn{uncommitted}.  An application or library can use
\code{madvise()} to uncommit a committed page, returning the
underlying physical memory back to the operating system.

{\paragraph{DLmalloc}} \cite{Lea96} is the default single-threaded
allocator in Linux.  DLmalloc is relatively simple and has been stable
for decades.  DLmalloc employs per-object boundary tags (which are due
to~\cite{Knuth73}).  On a 64-bit machine, each allocated object is
immediately preceded by 8 bytes which indicate the size of the object
(and hence the address of the next object, as well as whether the
object is allocated or free, and whether the previous object is
allocated or free.  When the previous object is free, the last 8 bytes
of that object also indicate the size.  When DLmalloc frees an object,
it can immediately coalesce the object with the next object (if it is
free) since it knows the size of the newly freed object.  If the
previous object is freed, it can also coalesce the newly freed object
with the previous one.  The boundary tags can contribute up to a 50\%
space overhead when allocating small (8-byte) objects.  DLmalloc
employs a first-fit heuristic, and has no provision for multithreading
performance beyond using a single lock to protect the entire
allocation data structure.  For example, DLmalloc has no per-thread
caching.

DLmalloc has been placed in the public domain.

{\paragraph{Hoard}} \cite{BergerMcBl00} was introduced during the era
of the first multithreaded allocators.  The original Cilk
allocator~\cite{BlumofeLe94} and the STL allocator of the
time~\cite{SGI97} provided per-thread allocation, which was very fast
(no locking was required), but also produced unbounded memory blowups.
Some allocators, such as PTmalloc~\cite{Gloger06} and
LKmalloc~\cite{LarsonKr98} exhibit memory blowup bounded by $P$, the
number of processors.  Hoard provides a provable bound on its space
blowup.  Hoard organizes small objects into \defn{chunks}, which
contain objects all the same size.\footnote{Chunks are called
  ``superblocks'' in \cite{BergerMcBl00}.  They are called ``chunks''
  in \cite{Evans06} and \cite{KukanovVo07}.}  (Large objects are dealt
with using heavier-weight mechanisms).  Hoard puts metadata at the
beginning of each superblock indicating the size of the objects in the
superblock.  Since the objects within a superblock are all the same,
Hoard does not need boundary tags.

Hoard uses an \defn{allocate-fullest} heuristic.  Hoard sometimes
finds itself in a situation where an object could be allocated out of
any one of several superblocks.  Hoard allocates the object out of the
fullest such superblock. By allocating into the fullest superblock,
Hoard improves the chances that a superblock will completely empty can
be reused for other purposes.

Hoard is licensed under GPLv2.

\begin{figure}
\begin{center}
\begin{tabular}{lrr}
Allocator & Bytes        & LoC \\ \hline
 DLmalloc    \cite{Lea96}        & 221K &  6,280 \\
 Hoard       \cite{BergerMcBl00} & 429K & 17,056 \\
 JEmalloc    \cite{Evans06}      & 624K & 22,230 \\
 TBBmalloc   \cite{KukanovVo07}  & 350K &  9,705 \\
 SuperMalloc                     & 127K &  3,934 \\
\end{tabular}
\end{center}
\caption{Code sizes measured in bytes and in Lines of Code (LoC).
  Note: TBBmalloc is tough to measure since it's mixed into TBB.
  SuperMalloc could end up with more code as it becomes
  production-quality.}
\label{fig:codesize}
\end{figure}

{\paragraph{JEmalloc}} \cite{Evans06} is, in our experience, the best
allocator now available for production.  JEmalloc ships with BSD and
Mozilla, as well as with MariaDB in recent Red Hat distributions.
JEmalloc strives to minimize footprint in long-running processes such
as browsers and database servers.  Like Hoard, JEmalloc allocates
large chunks of memory containing objects all the same size.  

Whereas Hoard uses an allocate-fullest heuristic, JEmalloc uses a
lowest-address heuristic: Of all the possible objects to allocate,
JEmalloc allocates the one with the lowest address.  This strategy is
something akin to a first-fit strategy, except that all the objects
are the same size.  First fit for fixed-size objects is generally
pretty good at freeing up entire blocks, but it seems likely that
fullest-fit is generally better.

JEmalloc uses a thread-local cache, and eventually returns objects
from the thread-local cache globally accessible memory.  JEmalloc is
faster than Hoard, and although JEmalloc does not provide any explicit
bounds on memory blowup, it seems to be the allocator of choice for
long-running multithreaded processes that need to keep their footprint
under control.

JEmalloc uses three size categories: small, large, and huge, and
implements them differently. For small and large objects, JEmalloc
carves a chunk into page runs using a buddy algorithm, and maintains
metadata at the beginning of the chunk.  By storing metadata at the
beginning of the chunk, the pages themselves can remain untouched, and
uncommitted.

JEmalloc directly maps huge objects \code{mmap()}.  Huge objects are
chunk-aligned.  JEmalloc maintains a red-black tree mapping huge chunk
addresses to metadata.

JEmalloc is licensed under a variant of the modified BSD license.

{\paragraph{TBBmalloc}} \cite{KukanovVo07} is the allocator shipped
with Intel's Threading Building Blocks (TBB), and is based on ideas
developed in McRT-malloc \cite{HudsonSaAd06}.  TBBmalloc is about as
fast as SuperMalloc, but has an unbounded space blowup in theory and
in practice.  TBB use a thread-private heap, and never returns space
for small objects to the operating system.  TBBmalloc always allocates
out of a per-thread pool with no locking.  If the same thread returns
the object as allocated it, then no locking is needed, otherwise the
object is being returned to a \defn{foreign} block, and requires
lightweight synchronization to place the object into a linked list
that will be dealt with the next time the owner of the foreign block
tries to allocate.  Like JEmalloc and Hoard, TBBmalloc uses chunks
each containing objects of homogeneous size, and places metadata at
the beginning of each chunk.

TBBmalloc can have an unbounded footprint.  One case was documented by
\cite{Vyukov08}.

TBBmalloc is licensed under GPLv2.

\secput{supermalloc}{SuperMalloc}

\secref{intro} reviewed the competition, and this section explains how
SuperMalloc works.   

Like the other chunk-oriented allocators (Hoard, JEmalloc, and
TBBmalloc), SuperMalloc allocates large chunks of homogeneous-sized
objects for small objects, and uses operating-system-provided memory
mapping for larger objects.  SuperMalloc's chunks are multiples of
$2$~MiB, and are $2$~MiB-aligned, which corresponds to huge pages on
x86.  In operating systems that support huge-pages, chunks align
properly with the paging system.  For objects that are significantly
less than $1$~MiB, SuperMalloc carves chunks into small pieces.  For
larger objects, the allocation is backed by one or more contiguous
chunks.

Unlike the other chunk-oriented allocators, SuperMalloc does not
divide its chunks into smaller pieces.  For example, JEmalloc divides
its chunks into runs using a buddy algorithm, and each run can contain
objects of a different size.  In SuperMalloc, the entire chunk is the
same size.  The rationale for this decision is that on a 64-bit
machine, virtual-address space is relatively cheap, while physical
memory remains dear.  The other allocators' approach of dividing up a
chunk saves virtual-address space by requiring fewer chunks to be
allocated.  In the case where that strategy actually does save space,
it is because the application did not actually allocate a whole
chunk's worth of objects of a given size.  In that case, in
SuperMalloc, the rest of the block is an untouched uncommitted
``wilderness area'' (a term apparently coined by \cite{KornVo85})
using no memory.

Like JEmalloc, SuperMalloc employs several object sizes.  The sizes
are organized as into bins.  In SuperMalloc, there are small, medium, large,
and huge objects.  The first 45 bins are specific sizes, and larger
bin numbers encode the number of pages that the application actually
allocated (so that when we map a 2~MiB chunk to support a 1.5~MiB
request, we can properly track the amount of physical memory that the
application has asked for).  A 4-byte bin number allows us to allocate
objects of up to just under $2^{32}$ pages ($2^{44}$ bytes).

Given a pointer to an object, a chunk-based allocator must be able to
determine which chunk an object belongs to, and what the size of the
objects in that chunk are.  Here, for comparison, we focus on the
comparison with JEmalloc for which the internal data structures are
explained in some detail.  In JEmalloc, depending on the size of the
object, the object might be looked up in a global red-black tree (for
huge objects), or in a buddy-allocation data structure at the
beginning of a chunk.  

SuperMalloc adopted a simpler strategy.  An entire chunk is all the
same sized, and we can look up the chunk number in a table.  Instead
of using a tree, SuperMalloc uses an array to implement that table,
however.  Since the usable x86 address space is $2^{48}$ bytes and
chunks are $2^{21}$ bytes, there are only $2^{27}$ entries in the
table.  Each entry is a 4-byte bin number.  The SuperMalloc table
consumes $512$MiB of virtual address space, but since the operating
system employs a lazy commit strategy, we typically need only a few of
pages physical memory for the chunk table.  This is another instance
of the design principle for 64-bit software that ``it is OK to waste
some virtual address space.''

\secput{small}{Small Objects}

Small objects are as small as 8 bytes, and increase in size by at most
25\% to limit internal fragmentation.  Small objects are regularly
spaced: the sizes are $8$, $10$, $12$, $14$, $16$, $20$, $24, \ldots,
224$, and their sizes take the form $k\cdot2^i$ for $4\leq k \leq 7$
and $1\leq i \leq 5$.  In other words, a small object size, when
written in binary is a $1$, followed by two arbitrary digits, followed
by zeros.  Thus bin~$0$ contains $8$-byte objects, bin~1 contains
$10$-byte objects, and so forth.  To compute the bin number from a
small size can be done with bit hacking in $O(1)$ operations using the
code shown in \figref{size2bin}.

\begin{figure}
\begin{minted}[mathescape]{c}
int size_2_bin(size_t s) {
  if (s <= 8) return 0;
  if (s <= 320) {
    // Number of leading zeros in $s$.
    int z = clz(s);
    // Round up to the relevant
    // power of $2$.
    size_t r = s + (1ul<<(61-z)) -1;
    int y = clz(r);
    // $y$ indicates which power of two.
    // $r$ shifted and masked indicates
    //  what the low-order bits are.
    return 4*(60-y)+ ((r>>(61-y))&3);
  }
  if (s <= 448) return 22;
  if (s <= 512) return 23;
  ..
  if (size <= 1044480) return 45;
  return 45 + ceil(size-1044480, 4096);
}
\end{minted}
\caption{Code to convert a size to a bin number.  The first 22 bins
  are handled by bit hacking. The \code{clz()} function returns the
  number of leading 0 bits in its argument. Bins~22--45 are handled by
  a case statement (the elided sizes are $704$, $832$, $1024$, $1088$,
  $1472$, $1984$, $2048$, $2752$, $3904$, $4096$, $5312$, $7232$,
  $8192$, $10048$, $14272$, $16384, 32768$, $65536$, $131072$,
  $258048$, and $520192$.)  Huge bins are computed as a constant plus
  the number of pages used by the object.}
\label{fig:size2bin}
\end{figure}

For small objects, we make no particular effort to avoid false
sharing.  Like JEmalloc, we assume that users who care about false
sharing will explicitly ask for cache-line alignment, using e.g.,
\code{posix_memalign()}.  This decision stands in contrast to
allocators, such as Hoard, that try to use temporal locality to induce
spatial locality: Hoard has heuristics that try to arrange to place
objects that were allocated at the same time by the same thread onto
the same cache line, and objects that were allocated by different
threads on different cache lines.  As explained below, we take a
different tack on avoiding false sharing, focusing on objects that are
larger than a cache line.

We reserve the first few pages of each 2~MiB chunk for bookkeeping.
The bookkeeping comprises a bitmap of the free objects and a heap-like
data structure for implementing the fullest-fit
heuristic. \bcknote{Explain how we implement fullest fit}

\secput{medium}{Medium objects:}

Medium objects are all sized be a multiple of 64 (the cache line
size).  The smallest is 256 bytes, and the largest is 14272 bytes.
The sizes we chose include powers of two and prime multiples of cache
lines.  The powers of two ($256$, $512$, $1024$, $2048$, $4096$,
$8192$) are used only when the application explicitly asks for aligned
data.  The prime multiples of cache lines are used for all other
requests to reduce the contention in the low-associativity processor
caches. For example, recent Intel processors provide 8-way associative
level~1 cache.  Following the lead of \cite{AfekDiMo11}, which propose
\defn{punctuated arrays} to reduce associativity conflicts, we wanted
to avoid aligning objects into the same cache associativity
set. (Apparently punctuated arrays are appearing in most of the
Solaris~12 allocators \cite{Dice14b}.)  We, however, took the more
``radical'' approach mentioned by \cite{Dice14a} of making all the
size classes a prime multiple of the cache line, which not only
reduces conflicts within a class size but reduces inter-size
cache-index conflicts.

One aspect that becomes relatively more important for medium objects
than small objects is what happens when the page size is not a
multiple of the object size.  Consider for example the 832-byte size
bin (832 bytes corresponds to 13 cache lines).  If we fill up a
4096-byte page with these objects, we would be able to fit 4 objects
into a page wasting 768 bytes at the end.  

Supermalloc avoids this internal fragmentation by using larger logical
pages, called \defn{folios} for allocation bookkeeping.  For example,
in the 832-byte bin, rather than wasting those 768 bytes, we allocate
objects out of a folio containing 13 pages (53,248 bytes). When we
allocate an 832-byte object, we find the fullest 13-page folio (among
all 832-byte folios in the system) that has a free slot, and allocate
that slot.  When a folio becomes empty, we uncommit the entire folio,
rather than trying to uncommit individual pages of a folio.  There is
still fragmentation at the end of the 2~MiB chunk (5 unused pages in
this case), but that fragmentation comprises whole pages which are
never touched and remain uncommited.  Once again, we do not mind
wasting a little virtual address space.

Except for the way their sizes are chosen, medium and small objects
are managed exactly the same way (with folios and the fullest-fit
heuristic, for example) by exactly the same code.

\secput{large}{Large objects}

Large objects start at 16KiB (4 pages) and go up to half a chunk size.
Large objects are all a multiple of a page size, and the code is a
little bit simpler than for small and medium objects, since it mostly
does not matter which large object is returned by \code{\malloc()};
When a large object is freed its pages can be uncommitted, so we don't
need to keep track of anything for a fullest-fit heuristic.

In order to avoid associativity issues, we add random number of cache
lines, up to a page's worth, to the allocation, and adjust the
returned pointer by that random number.  For example when asked to
allocate $20,000$ bytes we pick a random number from 0 to 63 and add
that many cache lines to the request.  If, for example, we picked 31
for the random number, we would allocate $22,048 = 20,000+31\cdot64$
bytes.  That would be allocated out of bin~40, which contains $32$KiB
objects.  Instead of returning a pointer to the beginning of the
$32$KiB object, we return the pointer plus $2048$ bytes.  This
strategy wastes at most an extra page per object, and since we use
this strategy only on objects that are $4$ pages or larger, it can
result in at most $25$\% internal fragmentation (which matches the
fragmentation we were willing to tolerate for the small objects).  If
the application explicitly asks for a page-aligned allocation, we skip
the random rigamarole.  Adding a random-offset to what would otherwise
be a page-aligned allocation appeared in \cite{LvinNoBe08} for a
page-per-object allocator, and is applied in a more systematic fashion
by \cite{AfekDiMo11}.

This approach introduces one big gap between $7$~and $11$~cache lines
introducing internal fragmentation of up to 57\% for an allocation of
449 bytes, and slightly smaller gap between 13 cache lines and 17
cache lines ($31$\% fragmentation for 833-byte allocations).  To close
these gap, we plan to add two more bins at 9 cache lines and 15 cache
lines, with the rationale that a little bit of inter-size
associativity conflict is better than internal fragmentation.  (We
haven't yet made this change, but it should not affect any of the
performance results shown below.)

\secput{huge}{Huge objects}

Huge objects start at half a chunk (about 1MiB) and get larger.  The
SuperMalloc huge object allocation primitive is straightforward, and
is similar to many other allocators.  We use operating system
primitives such as \code{mmap()} to create new huge chunks, and we
never give the memory back using \code{unmap()}.  We do take advantage
of the virtual-addresses-are-cheap observation however.

We round up huge objects to the nearest power of two.  For example to
allocate a $5$MiB object, we allocate an $8$MiB region (which is
$2$MiB-aligned) of virtual memory using \code{mmap()}. 

We keep a linked list for each such power of two, so that when we
\code{free()} a huge chunk, we add it to the linked list.  We actually
thread the linked list through the chunk table so that the chunk
itself does not need to be in committed memory.  When we free a huge
object we explicitly uncommit the memory using \code{madvise()}.

We use \code{madvise()} to ask to the kernel to map the huge chunk
using huge pages (recent Linux kernels support $2$~MiB pages, which
can reduce TLB pressure and reduces the cost of walking the page table
in the case of a TLB miss \cite{Corbet11}).  If the object is not a
multiple of $2$MiB in size, the last fractional part of the huge page
is mapped only with normal $4$KiB page table entries.

Thus, although when we allocated $8$MiB to implement a $5$MiB
allocation, only $5$MiB is actually every committed to memory (and
then only when the application code touches it.)

\secput{arith}{Arithmetic}

One common operation in SuperMalloc is to calculate indexes in a
bitmap.  For example, given a pointer $p$, we must perform calculation
in \figref{bitindex}(a) to calculate which folio and which object in
the folio $p$ refers to.

\begin{figure*}
\begin{minted}[mathescape,linenos,xleftmargin=1cm]{c}
C = p / chunksize;          // Compute chunk number.
B = chunkinfos[C];          // What is the bin?
FS = foliosizes[B];         // What is the folio size?
FN = (p%chunksize)/FS;      // Which folio in the chunk?$\label{li:FN}$
OS = objectsizes[B];        // How big are the objects?
ON = ((p%chunksize)%FS)/OS; // Which object number in the folio?$\label{li:ON}$
\end{minted}
\begin{center}
(a)
\end{center}
\begin{minted}[mathescape,linenos,xleftmargin=1cm]{c}
C = p / chunksize;
B = chunkinfos[C];
FS = foliosizes[B];
FN = ((p%chunksize)*magic0[B]) >> magic1[B];       // magic
OS = objectsizes[B];
ON = ((p%chunksize-FN*FS)*magic2[B]) >> magic3[B]; // magic
\end{minted}
\begin{center}
(b)
\end{center}
\caption{The calculation to compute the folio number in the chunk,
  \code{FN}, and the object number in the folio \code{ON}, so that the
  bitmap for the free objects in the folio can be updated.  (a) shows
  the code with expensive divisions in \lireftwo{FN}{ON}.  (b) shows
  the code with the divisions replaced by multiplication and shift.}
\label{fig:bitindex}
\end{figure*}


Since the chunk size is a power of two, computing the chunk number is
easy.  Computing the folio number requires dividing by the folio size,
which isn't a power of two, and isn't known at compile time, however.
Division is slow, so we follow the approach of
\cite{MagenheimerPePe87} to convert division to multiplication and
shift.  For example, for 32-bit values of \code{c}, we have
\mintinline{c}{x / 320 == (x*6871947674lu)>>41}.  \figref{bitindex}(b)
shows the faster code.  We use metaprogramming to generate all the
object sizes (e.g., to find prime numbers that are spaced no more than
25\% apart, and to calculate the multiplication and shift constants to
implement division.).

\secput{caching}{Caching}

Like other multithreaded allocators, SuperMalloc employs per-thread
caching.  In addition to the per-thread cache SuperMalloc also employs
a per-CPU cache, as well as a global cache in front of the true data
structures.  Each cache is size-segregated (that is, there is a cache
for each size bin), but we'll describe it here as though there were
only one size.  Each cache comprises a small number of doubly-linked
lists of objects that belong in its bin.  A single doubly-linked list
can be moved from one level of the cache to the other in $O(1)$
operations, so that we can minimize the length of the critical section
that moves the list.

The cost of locking and accessing a shared data structure turns out to
be mostly due to cache coherence traffic from accessing the object,
rather than the locking instructions.  \figref{overhead} shows the
overheads for incrementing a global variable protected by a global
lock when multiple threads are running.  For example on our E5-2665,
that was $485.5$ns.  One alternative is to use a thread-local variable
which costs only $3.1$ns.  Most modern allocators use a thread-local
cache of global values.  But it turns out that a per-CPU data
structure costs only $34.1$ns to access.  The difference is that the
per-CPU data structure mostly resides in the right cache (it's in the
wrong cache only when the operating system migrates a thread to a
different processor), and the lock instruction is mostly uncontended
(contention happens only when the operating system preempts a thread).
So instead of paying for all the cache-coherence traffic, we pay only
the cost executing the lock instruction and the increment instruction.
For the per-CPU cache we also must determine which CPU we are on, for
which we use a call to \code{sched_getcpu()}.  If we factor out the
cost of \code{sched_getcpu()}, the per-CPU data structure costs only
$13.3$ns.  Based on these numbers, we concluded that a per-CPU cache
gets most of the advantage of a per-thread cache, but that we still
need a small per-thread cache.  Uncontended locking is not so bad.

A related approach used by some systems is to hash the thread
identifier instead of using a per-CPU cache.  The hash is then used to
index a collection of not-quite-thread-and-not-quite-per-CPU caches.
This hashing approach typically requires more caches than there are
CPU's, however, and can be analyzed as follows.  If there are $P$
threads running,\footnote{There may be more threads actually running,
  but at least for a scheduling quantum, only $P$ of them are actually
  scheduled.} $P$ processors, and $P$ caches, we still expect quite a
bit of contention on some of the caches.  In fact, we expect only
about $P/e$ threads to operate without contention due to a standard
balls-and-bins argument (and some cache will have contention about as
bad as $\log P/\log\log P$).  Since the cost of contention is so high,
this would reduce the cost of access to no less than
\[ 485.5/e + 34.1\cdot(1-1/e) = 212\mbox{ns},\]
which is still an order of magnitude slower than the uncontended
per-CPU operation.  To get the contention close to zero, we can apply
the birthday paradox, and conclude that we might need $P^2$ caches.
That's probably overkill, since if we go the contention down to, say
$1/20$ of the accesses, we would get rid of enough contention to get
to within about half of optimal.  To get the contention to $1/20$
requires approximately $3P$ caches.  (JEmalloc uses $4P$ caches.)  The
biggest advantage to using only $P$ per-CPU caches instead of $3P$ (or
$P^2$) caches indexed by a hash of the thread index, is that the cache
takes less space.  All the objects in the caches are likely to be
using committed memory, and we want to waste as little physical memory
as possible.

\begin{figure}
\begin{tabular}{lrrr}
                             & i7-4600U & i7-4770 &  E5-2665 \\
                             & 2 cores  & 4 cores &  16 cores \\
                             &          &         & 2 sockets \\
                             & 2.1GHz   &  3.4Ghz &  2.4GHz \\ \hline
Global                       &  149.0ns & 120.6ns & 485.5ns \\
Per-CPU                      &   29.6ns &  18.0ns &  34.1ns \\
Per-CPU no getcpu            &   17.2ns &  10.6ns &  13.3ns \\
Per-thread                   &    3.3ns &   2.1ns &   3.1ns \\
\end{tabular}
\caption{Lock vs.~contention overhead.  ``Global'' is the cost of
  accessing a global lock and incrementing a global counter.
  ``Per-CPU'' is the cost of call \code{sched_getcpu()} and accessing
  a per-cpu lock and increment a counter.  ``Per-CPU no getcpu'' is
  same thing without the call to \code{sched_getcpu()}.  Per-thread is
  the cost of incrementing a thread-local variable without a lock .}
\label{fig:overhead}
\end{figure}

The SuperMalloc per-thread cache is just big enough to amortize the
$34$ns uncontended locking overhead compared to the $3.1$ns unlocked
overhead, so we want about $10$ objects in the per-thread cache.  We
decided to store two linked lists each containing up to 8~KiB worth of
objects for each bin in the thread cache.

We tried to tune the per-CPU cache to be just big enough to fill up
the processor's data cache.  We reasoned that that if the working set
of a thread is bigger than the data cache, the thread will be
incurring cache misses anyway. In this case, there is no point in
working hard to avoid a few more cache misses.  In contrast, many
allocators keep many megabytes in their per-thread caches.  Because
there are many per-thread caches (thousands of threads in a typical
database server, for example) their footprint is larger.  We chose to
store two linked lists each containing of to $1$MiB for each bin in
the per-CPU cache.

There is also a global cache containing $P$ linked lists of up to
$1$MiB each. 

The logic for allocating an object is as follows:
\begin{enumerate}
\item If the thread cache has an item, use it.  This requires no locking. \label{step:thread}
\item Otherwise if the CPU cache contains a non-empty linked list, move it to the thread cache, and proceed to step~\ref{step:thread}.  Moving an item from the CPU cache requires mutual exclusion, which is implemented with a transaction on chips that support transactions (such as Haswell) otherwise with locking. There is a lock for each cpu-bin pair.\label{step:cpu}
\item Otherwise if the global cache contains a non-empty linked list, move it to the thread cache, and proceed to step~\ref{step:thread}.  This step also requires mutual exclusion (with a transaction or a lock), and there is a lock for each bin.
\item Otherwise access the global data structures, which requires mutual exclusion (with a transaction or a lock), and there is a lock for each bin.
\end{enumerate}

The logic for deallocating an object is similar:
\begin{enumerate}
\item If one of the linked lists in the thread cache isn't ``full'', then add the object to that list.
\item Otherwise, if one of the CPU caches is empty, move one of the linked lists from the thread cache to the CPU cache (add the object to that list while we are at it.)
\item Otherwise, if one of the global caches is empty, move one of the linked lists from the thread cache to the global cache (add the object to that list while we are at.)
\item Otherwise access the global data structures to free the object.
\end{enumerate}

Most of the critical-sections are on the CPU cache, and since there is
no guarantee that we stay on the same CPU for the duration of a
critical section, we need locks.  The CPU cache employs a lock for
each CPU-bin pairing.  Since there are 45~bins and 32~CPU's on the
biggest machine we measured, that's 1,440~locks in use.  We place each
lock on its own cache line.  We considered placing the locks and the
doubly linked list heads into the same cache line, but concluded that
that might cause performance trouble.  For example, on the HTM
hardware, we spin on the lock before starting the transaction.  But
that spin could cause the transaction to fail if the lock were on the
same cache line as the data being modified.  Without HTM, spinning on
the lock could interfere with the updates being made to the linked
lists by causing the cache line to bounce around.

For the global data structure, we have a lock for each bin.  Since we
seldom access the global data structure, the lock is mostly
uncontended.

When running on hardware that supports HTM, we use transactions.  If
the transaction fails enough, we fall back to locking code using the
original lock structures.  To make the HTM transactions interact
properly with the lock-protected transactions, the HTM transaction
must look at the lock state, and verify that it is unlocked.  This
lock examination is called \defn{subscribing} to the lock.  One
decision to make is whether to subscribe to the lock early or late in
the transaction.  Subscribing late sounds enticing because it reduces
the time during which we conflict on the lock (when in fact we might
not conflict on the data structure).  Subscribing late can be
dangerous, however, since it can be difficult to prove that your code
does not get tricked into committing \cite{DiceHaKo14}.  We subscribe
to the lock early in the transaction for three reasons.  (1) We do not
feel confident that we can guarantee that the code avoids all the
pitfalls of late subscription.  (2) Our data structures are closely
related to our locks.  It's unlikely that the lock conflicts but that
we don't actually conflict on the data.  (3) Our transactions are all
so short that it probably doesn't matter.

In order to make the critical sections short, both for HTM and
locking, we try to prefetch everything that will be accessed, then
wait for the lock to become free (and if the lock wasn't free, we
prefetch everything again, and check the lock again.)  The goals is to
make it so that while the lock is held (or the transaction is
running), as few cache misses as possible are incurred.  This
prefetching improves performance by a few percent (see
\figref{prefetchhelps}).

Need better spin waiting on locks that we will subscribe to.

\secput{performance}{Performance}

We are interested in three aspects of performance: speed, footprint, and complexity.

{\paragraph{Speed:}} A poor allocator can be slow even on serial
applications, and the situation can get far worse on multithreaded
codes where the allocator can become a serial bottleneck.  On a
multicore with a multithreaded workload the speed difference between
the default allocator in Linux \cite{Lea96} and a state-of-the-art
allocator such as JEmalloc~\cite{Evans06} can be more than a factor of
30.  If the cost of allocation varies significantly it can be
difficult to meet the soft real-time constraints of a server.  We have
seen reports of \code{malloc()} causing a 3~second pause about once
per day on a database server, which would be too slow for a
social-networking site.  It was this 3~second pause that led us to
investigate how to achieve the performance of JEmalloc with less
variance.

\begin{figure}
\input{new-malloc-test-1K-lutestring-aggregated}
\caption{A comparison of SuperMalloc, DLmalloc, Hoard,
  JEmalloc, and TBBmalloc running malloc-test
  on a 4-core (1 socket + hyperthreading) 3.4GHz i7-4770 (Haswell-DT)
  The lines are the average of 8 trials, and the error bars
  show the fastest and slow trial.}
\label{fig:data}
\vspace*{-3ex}
\end{figure}


\begin{figure}
\input{new-malloc-test-1K-tempo-aggregated}
\caption{A comparison of SuperMalloc, DLmalloc \cite{Lea96}, Hoard
  \cite{BergerMcBl00}, and JEmalloc~\cite{Evans06} running malloc-test
  on a 16-core (2 sockets + hyperthreading) 2.4GHz E5-2665 (Sandy
  Bridge).  The lines are the average of 8 trials, and the error bars
  show the fastest and slow trial.}
\label{fig:data}
\vspace*{-3ex}
\end{figure}

\figref{data} compares the performance of SuperMalloc against
DLmalloc, Hoard, and JEmalloc on the malloc-test
benchmark~\cite{LeverBo00}.  The malloc-test benchmark runs $K$
producer threads and $K$ consumer threads.  Each producer thread
allocates objects as fast as possible, and each consumer frees them.
Malloc-test offers one of the most difficult workloads for
multithreaded allocators that employ per-thread caching, since the
per-thread caches can quickly become unbalanced.

{\paragraph{Footprint:}} The \defn{memory footprint} of an
application, the amount of physical memory that the application
consumes while running, can also vary by more than an order of
magnitude --- even a factor of two can be too much on something like a
database server where memory is used as a cache for disk, and an
increased footprint results in either a reduced effective cache size
or excessive I/Os due to paging.  Both JEmalloc~\cite{Evans06} and Hoard~\cite{BergerMcBl00} 

{\paragraph{Complexity:}} A simple memory allocator can operate using
only a few hundred lines of code \cite{KernighanRi88}.  Since
allocator performance is so complex, however, most allocators have
been tuned to run faster at the cost of increased complexity.  The
code sizes of \figref{codesize} show that SuperMalloc is smaller, but
where does the code complexity come from in these allocators?

\figref{supermodules} shows the largest modules in SuperMalloc.  The
biggest module is for managing the cache.  The interface to make the
standard POSIX calls is second biggest (including functions such as
reallocation, and testing).  The code for actually performing
allocation in the three size classes (small objects, medium/large objects, and huge objects)
together is just a little larger than the cache management code.  The
code for invoking locks or hardware transactional memory, and the
metaprogram that computes the bin sizes and sets up all the constants
are each also about 350 lines of code.  The code for interfacing to
mmap to get chunk-aligned memory is 123 lines.

\begin{figure}
\begin{center}
\begin{tabular}{lrr}
Module & Characters & LoC \\ \hline
mmap-interface         &   5,066 &  123 \\        
huge objects           &   7,312 &  189 \\
medium \& large objects&  10,203 &  302 \\
metaprogram            &  14,791 &  332 \\
atomicity and locks    &   9,296 &  357 \\
small objects          &  17,096 &  461 \\
Front end (API)        &  17,256 &  536 \\
Cache management       &  25,340 &  841 \\
\end{tabular}
\end{center}
\caption{The sizes of the significant modules of SuperMalloc, sorted by size.}
\label{fig:supermodules}
\end{figure}

For JEmalloc, the biggest module manages their ``arenas'' at 2,577
LoC, which implements the meat of their code, basically implementing
the functionality of objects modules and the cache management (which
for us adds up to 1,793 LoC).  
One significant module in JEmalloc, in
terms of lines of code, manages the tuning parameters (giving the user
control, for example, of the number of arenas and providing access to
various counters).  Although that code has many lines of code, it is
not complex.  It's just long because there are many tuning parameters.
We do not want tuning parameters in SuperMalloc, preferring instead
that the code work well in all cases.  It may turn out that our
determination to avoid tuning parameters will fade away when faced
with the demands of production --- we should chalk up this part of
JEmalloc's complexity to the fact that it's production-quality code.

Hoard contains a huge number of modules, each of which is only a few
hundred lines of code, which can be used to build good special-purpose
and general-purpose allocators \cite{AlexandrescuBe05}.  SuperMalloc
isn't trying to enable to construction of special-purpose allocators,
so our code is smaller.

\secput{wishlist}{An Operating-System Wish List}

There are two features that Linux does not provide which are provided
by some other operating systems: a cheaper way to uncommit memory, and
a hook to reduce the odds that a thread is preempted while holding a
lock.

{\paragraph{Cheap uncommit:}} The first problem is to uncommit memory
cheaply.  Linux provides a function
\begin{center}
\code{madvise(ptr, len, MADV_DONTNEED)}
\end{center}
which informs the operating system that the \code{len} bytes of memory
starting at address \code{ptr} are no longer needed.  For anonymously
mapped memory (the kind that \code{malloc()} uses), this removes the
page from the page table, freeing the physical page.  The next time
the process touches the page it will cause another page fault, at
which point the operating system will allocate and zero-fill a
physical page.  That is a fairly expensive operation --- it takes
$1800$ns to call \code{madvise(MADV_DONTNEED)} to uncommit a page,
whereas if the page is already uncommited it costs only $240$ns to
make the same system call.

One potential alternative is to use
\begin{center}
\code{madvise(ptr, len, MADV_FREE)},
\end{center}
which is provided by FreeBSD and Solaris.  The semantics of this call
is to give the kernel freedom to uncommit the page the way
\code{MADV_DONTNEED} does at any time until the process touches the
page again.  The kernel also has the freedom not to touch the page, so
that the data could still be the same data.  Since the kernel is in a
position to understand memory pressure as well as the overall system
load, it can decide when and whether to spend the CPU cycles to
reclaim memory.

Evans complains that ``\code{MADV_FREE} is \textit{way} nicer than
\code{MADV_DONTNEED} in the context of malloc.  JEmalloc has a really
discouraging amount of complexity that is directly a result of working
around the performance overhead of \code{MADV_DONTNEED}''
\cite{Evans12}.  We agree with Evans' sentiments, although the
SuperMalloc caches reduce the impact of the \code{madvise()} calls.
Although SuperMalloc would not get much simpler with \code{MADV_FREE},
it would speed up and exhibit less performance variance.

An alternative approach would be for the kernel to deliver an event to
a process indicating that it should use less memory.  The allocator
generally has a big list of empty pages lying around that it could
quickly return to the kernel.

{\paragraph{Lock-aware scheduling:}} The second problem shows up when
we are running in locking mode (that is, without transactional
memory).  Sometimes while a lock is held by a thread, the kernel
preempts the thread and schedules something else.  Since the thread
holds a lock, any other threads that try to acquire that lock suspend
too, which can lead to lock convoying and other performance problems.

Solaris provides one way to fix this using the \code{schedctl()}
system call, which briefly tells the kernel not to, if possible,
preempt a thread \cite{Dice11}.  The mechanism includes two steps: (1)
the thread says it is about to enter a critical section, and (2) the
thread indicates that it has left the critical section.  Step (1) is
accomplished by setting a bit in a thread-specific structure.
Meanwhile the kernel, if it wanted to preempt the thread, sets a bit
in the structure.  Step (2) is accomplished by checking to see if the
``wanted-to-preempt'' bit is set, and if so, politely yielding.  The
kernel must take some care to downgrade threads that abuse the
mechanism.

The Linux maintainers seem skeptical of this feature.  For example,
see \cite{Aziz14}, which proposed the patch and claimed that it
improves TPC-C performance by 3\%--5\%, but the kernel maintainers
dismissed it as ``a feature for a real-time kernel'' and suggested a
voluntary preemption model instead, and that they were ``skeptical
about the whole idea''.  One proposed \cite{Oboguev14a, Oboguev14b}
that a userspace mechanism to set the priority of a thread would solve
the problem.  As implementers of \code{malloc()}, we do not see how it
would --- \code{malloc()} is in a library, and we do not know what, if
any, thread priority schemes the application may be using.

We would like to see a hook in Linux to avoid preempting threads that
are briefly holding a lock.

--------------

* Tradeoff: Code size and complexity, speed, footprint, variability.  Supermalloc appears to disprove this tradeoff.
* Allocators use trees, buddy systems: supermalloc doesn't.
* Work for HTM and locks.


Notes: 
\begin{itemize}
%\item \cite{ReinefeldDoSc13} shows the importance of malloc.
% \item \cite{Dementiev09} shows an anecdote about the importance of caching even fairly large objects (200KiB).  He claims that the lazy-commit strategy was causing nonscalability.  I did an experiment, and it looks like his analysis is wrong (possibly superseded by newer kernels?)
\item \cite{KukanovVo07} their cross-test sounds similar to malloc-test
\item \cite{DetlefsDoZo94} Benchmarks include gs running on a 126-page manual.
\item \cite{Michael04} shows a lock-free mallocator based on Hoard, with some interesting benchmarks: Linux-scalability, threadtest, larson and a producer-consumer test.
\item \cite{FengBe05}
\item \cite{SchneiderAnNi06}: streamflow (worries about cache behavior by using segregated heaps).  Synchronization-free operations for local alloc/free, and non-blocking synch for remote deallocations.  [In contrast, SuperMalloc doesn't care as much about trying to get cache behavior to be local.  Perhaps wrongly so.  Supermalloc does worry about false sharing  ].  Does poorly on memory footprint on the Larson benchmark, but they didn't isolate the issue, since they ran Larson for a fixed time instad of a fixed amount of work.
\item \cite{LarsonKr98} LKmalloc hashes the threadid (so it doesn't need so many arenas, but it still needs a lock.)  They say for servers the requirements are to be fast, high-memory utilization, size indenpendent, locality, scalable, thread independence, predictable speed, stability).  LKmalloc is similar DLmalloc in that it uses bins of free blocks, approx best fit, and immediate coaslescing.  LKmalloc improves on DLmalloc by providing a lock per free list, multiple subheaps (to reduce cache sloshing (a value migrating around), select subheap by hasing, and striping (4MB pieces allocated to a heap)
\item \cite{Gloger12} presents measurements of several allocators, with an emphasis on ptmalloc.
%\item \cite{Gloger06} is the web page for ptmalloc, an allocator based on dlmalloc \cite{Lea96}.
\item knary a benchmark we should use (since I helped write
  it...).  Knary was used in \cite{SchneiderAnNi06}, but was
  attributed to Hood when in fact it's due to \cite{BlumofeJoKu96}.
\end{itemize}




%SuperMalloc uses simple data structures to try to maximize the odds that hardware transactions succeed. For example, the SuperMalloc uses a priority heap to allocate objects out of the fullest page. The heap takes advantage of the fact that for each class, there are only a relatively small number of possible page-fullness values. For example, for 8-byte objects, there are only 512 objects in a page, and so instead of using a general heap, SuperMalloc uses an array of 513 lists, the $i$th list containing a list of pages with $i$ free slots.



%SuperMalloc uses a per-CPU cache and a per-thread cache. A per-thread cache of objects reduces the overhead of allocating and freeing objects because some objects do not need to perform any mutual exclusion. It turns out that most of the cost of updating a global data structure is due to cache misses, not the locking itself, however. On Haswell, an unlocked update to an uncontended cache line costs 3--5ns, whereas acquiring a lock and modifying uncontended variable costs 80ns. Acquiring an uncontended lock and updating an uncontended page costs only about 18ns. (On a multisocket Sandy Bridge processor, the difference is even more striking: 3ns uncontended unlocked, 26ns uncontended locked, and 460ns contended locked.) Accordingly, SuperMalloc uses a relatively small per-thread cache (containing only a few objects of each size), and uses a per-CPU cache (a cache per hardware thread). The per-CPU cache contains only one L3-cache worth of objects for each size class, since we assume that the application will actually store data into allocated objects. If the objects in the per-CPU cache don't fit in the L3 cache, then filling the objects will cause cache misses anyway, so it does not matter whether allocating the objects causes a few cache misses.

% To further improve the odds of transactions committing, SuperMalloc tries to prefetch into cache all the data of the transaction. Prefetching data appears to improve performance by about 5%.

%SuperMalloc appears to enjoy a substantially smaller footprint than the other allocators for two reasons. (1) SuperMalloc adopts Hoard's allocate-in-fullest-page heuristic rather than JEmalloc's approach of allocating the object with the lowest address. (2) SuperMalloc's caches are smaller than Hoard's or JEmallocs, mostly because the per-thread cache is extremely small, and in many applications there are far more threads than there are CPU's.

% SuperMalloc is currently implemented, and we are assembling and running the allocation benchmarks mentioned in other allocation papers. We plan to release the software under the Apache 2.0 license, and the assembled benchmarks under an appropriate mix of licenses.

SuperMalloc, like Hoard, allocates objects out of the fullest possible page in order to reduce memory footprint.

madv\_dontneed (and other wishlists from the talk)

I will soon release SuperMalloc under an open-source license.

{\raggedright
\bibliographystyle{abbrvnat}
\bibliography{allpapers}
}

%% \begin{figure*}
%% \begin{tabular}{cc}
%% \input{new-malloc-test-1K-tempo-aggregated}
%% &
%% \input{new-malloc-test-1K-lutestring-aggregated}
%% \end{tabular}
%% \caption{A comparison of SuperMalloc, dlmalloc, Hoard, and jemalloc on the malloc-test benchmark.}
%% \label{fig:data}
%% \end{figure*}

\end{document}

%%  LocalWords:  SuperMalloc threadcount multithreaded implementers
%%  LocalWords:  allocator allocators JEmalloc HTM DLmalloc malloc ns
%%  LocalWords:  uncontended superblocks TBB TBBmalloc GPLv LKmalloc
%%  LocalWords:  superblock PTmalloc Cilk multithreading multicore
%%  LocalWords:  metadata unallocated uncommit madvise scalability
%%  LocalWords:  SuperMalloc's codesize mmap STL supermalloc Solaris
%%  LocalWords:  uncommited associativity rigamarole metaprogramming
%%  LocalWords:  incrementing prefetch deallocating prefetching
%%  LocalWords:  transactional atomicity
