\documentclass[xcolor=dvipsnames,14pt]{beamer}
\beamertemplatenavigationsymbolsempty%Turn off the navbar
\usepackage{geometry}
\usepackage{minted}
\usepackage{graphicx}

\begin{document}
\title{SuperMalloc}
\author{Bradley C. Kuszmaul}
\date{Supertech Meeting Oct 6, 2014}
\frame{\titlepage}

\begin{frame}[fragile]
\frametitle{Malloc and Free}
\begin{minted}[mathescape]{c}
void* malloc(size_t s);
\end{minted}

Effect: Allocate and return a pointer to a block of memory containing at least $s$ bytes.

\begin{minted}{c}
void free(void *p);
\end{minted}

Effect: $p$ is a pointer to a block of memory returned by \mintinline{c}{malloc()}.  Return the block of memory of memory to the system.
\end{frame}

\begin{frame}[fragile]
\frametitle{Aligned Allocation}

\begin{minted}[mathescape]{c}
void* memalign(size_t alignment, size_t s);
\end{minted}


Effect: Allocate and return a pointer to a block of memory containing at least $s$ bytes.  
The returned pointer shall be a multiple of \mintinline{c}{alignment}.  That is,
\begin{center}
\mintinline{c}{0 == (size_t)(memalign(a, s)) % a}
\end{center}

Requires: \mintinline{c}{a} is a power of two.
\end{frame}

\begin{frame}
\frametitle{Goals For Malloc (Doug Lea)}

Maximize
\begin{description}[Error Detection:]
\item[Compatibility:] POSIX API.

\item[Portability:] SuperMalloc isn't.
\item[Space:] SuperMalloc wins.
\item[Time:] Average? Worst-case?  

SuperMalloc wins on average.
\item[Tunability:]

I hate tunability. Just make it good.
\item[Locality:] SuperMalloc doesn't.
\item[Error Detection:] SuperMalloc doesn't.
\item[Anomalies:] ???
\end{description}
\end{frame}

\begin{frame}
\frametitle{DLmalloc}

Linux libc employs Doug Lea's malloc, which dates from 1987.

\begin{itemize}
\item is slow (especially on multithreaded code).
\item has high space overhead.
\end{itemize}

To address these problems, allocators such as Hoard, TCmalloc, JEmalloc have appeared.

\end{frame}

\begin{frame}
\frametitle{DLmalloc Employs Bins}
A bin is a doubly linked list of free objects that are all close to the same size.

\end{frame}

\begin{frame}
\frametitle{DLmalloc Employs Boundary Tags}

Put the size before every object and after every free object.  The
size also indicates the status of the object and the previous
object. (``status'' means free or in-use.)

\begin{center}
\begin{tabular}{l|l|}
                                                    \hline
an allocated chunk & size                         \\ \cline{2-2}
                   & $\ldots$ user data $\ldots$ \\ \hline
a free chunk       & size                         \\ \cline{2-2}
                   & pointer to next chunk in bin \\ \cline{2-2}
                   & pointer to prev chunk in bin \\ \cline{2-2}
                   & $\ldots$ unused space $\ldots$ \\ \cline{2-2}
                   & size                        \\ \hline
an allocated chunk & size                        \\ \cline{2-2}
                   & $\ldots$ user data $\ldots$ \\ \hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{DLmalloc malloc()}

\begin{itemize}
\item Find any object in the smallest nonempty bin that is big enough.

\item If none available, get  more memory from operating system.

\item Historically: Earlier versions of dlmalloc implemented first-fit within each bin.

They kept the bins sorted (but maintaining a heap in each bin would
have been enough).

\item Now first-fit has been thrown out.
\end{itemize}
\end{frame}
  
\begin{frame}
\frametitle{DLmalloc free()}

\begin{enumerate}
\item Remove adjacent free blocks (if any) from their bins.
\item Coalesce adjacent free blocks.
\item Put the resulting free block in the right bin.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{DLmalloc is simple, but slow}

\begin{tabular}{rrr}
         & lines of code & malloc\_test speed \\
dlmalloc &    6,281 &  3.0M/s \\
hoard    &   16,948 &  5.2M/s \\
jemalloc    & 22,230 & \\
supermalloc & 3,571 & 15.1M/s \\
\end{tabular}

\vfill

malloc\_test allocates objects in two threads and frees them in
two others.  ``Speed'' is mallocs per second.

\end{frame}

\begin{frame}
\frametitle{DLmalloc suffers an 8-byte/object overhead}

Since each object is preceeded by an 8-byte size, there is a 100\%
overhead on 8-byte objects.

\end{frame}

\begin{frame}
\frametitle{DLmalloc suffers fragmentation}

\begin{itemize}
\item DLmalloc, in the past, implemented first-fit, but does not appear to do so now.

\item DLmalloc maintains ``bins'' of objects of particular size ranges.

\item Small objects end up next to large objects.

\item Pages can seldom be returned to the operating system.

\item Compared to Hoard or jemalloc, dlmalloc results in twice the
  resident set size (RSS) for long-lived applications, such as
  servers.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{How Hoard runs faster than dlmalloc}

\begin{itemize}
\item dlmalloc uses a monolithic lock.
\item Hoard employs per-thread caches, to reduce lock-acquisition frequency.
\item jemalloc uses many of the same tricks.  I'll focus on jemalloc
  from here on, since it seems faster, and I understand it better.
\item Each thread has a cache of allocated objects.
\item Each thread has an ``arena'' comprising chunks of each possible size.  When the thread cache is empty, the thread allocates out of its arena.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{How jemalloc runs smaller than dlmalloc}

\begin{itemize}
\item Allocate 4MiB chunks using \mintinline{c}{mmap()}.
\item Objects within a chunk are all the same size. 
\item The system suffers $1$-bit overhead per object.
\item Use a red-black tree indexed on the chunk number, \mintinline{c}{p >> 22}, to find a chunk's object size.
\item jemalloc allocates the object with the smallest address in an arena,
which tends to empty out pages.
\item Hoard is similar, except that it appears to allocate the object from the fullest page in the arena.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Returning Pages to the Operating system}

\begin{itemize}
\item Empty pages can be released using
\mintinline{c}{madvise(p, MADV_DONTNEED, ...)}, which zeros the page while keeping the virtual address valid.

\item jemalloc includes much complexity to overcome the high cost of
  \mintinline{c}{DONTNEED}.

\item Supermalloc may not suffer as much.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Performance of returning memory}

\begin{itemize}

\item On linux, avoid calling \mintinline{c}{munmap()}, which pokes
  holes in the virtual address space.  Linux has a slow algorithm for
  finding contiguous free virtual addresses.

\item BSD offers \mintinline{c}{madvise(p, MADV_FREE, ...)} which
  gives the kernel permission to free memory.  In future, the memory
  might be zero, or it might be the old value.
  \mintinline{c}{MADV_FREE} is much faster, since it often avoids both
  deallocating memory and subsequently reallocating it.

\item Supermalloc may not care.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Large objects}
\begin{itemize}
\item For objects $>$ 4MiB, round up to a 4MiB boundary.
\item If you \mintinline{c}{malloc(1+(1<<22)} (slightly more than 4MiB), the system allocates 8MiB.  
\item This allocation uses up virtual memory space, but not RSS\@.
  The operating system doesn't \textit{commit} physical memory for a
  page until the application reads or writes the page.  Since the page
  size is 4KiB, in this example, the RSS would end up be 4MiB+4KiB.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Costs of Locking vs. Cache Contention}

\begin{tabular}{rr}
                                                     global list &    193.6ns \\
            per cpu (always call \mintinline{c}{sched_getcpu()})  &     30.2ns \\
per cpu (cache getcpu, refresh/32) &     17.0ns \\
                                                      per thread &      3.1ns \\
                                                  local in stack &      3.1ns \\
\end{tabular}
\end{frame}

%% \begin{frame}
%% \frametitle{Supermalloc Strategies}

%% \begin{itemize}
%% \item Virtual address space is $2^{48}$ bytes.  Real memory is typically $2^{33}$ up to $2^{41}$ bytes. 

%% Don't waste real memory.  Do waste virtual memory.

%% \item Locking uncontended locks is far cheaper than accessing contended cache lines.

%% Use a per-CPU cache.

%% \item Applications access their objects, incurring cache misses.

%% Set the per-CPU cache to be smaller than L3.

%% \item Thread cache reduces locking costs.

%% Thread cache holds only about 10 objects.

%% \item HTM likes simple data structures.

%% Use arrays instead of red-black trees.

%% \item Associativity conflicts cause trouble.

%% Object sizes should be a prime number of cache lines.

%% \end{itemize}
%% \end{frame}


\begin{frame}
\frametitle{Supermalloc Strategies}

\begin{itemize}
\item Use up address space, it's $2^{48}$ bytes.
\item Don't use up RSS ($2^{40}$ bytes on big machines).
\item Contention costs dearly, not locking.  Use a per-CPU cache.
\item Make per-CPU cache smaller than L3 cache, since the application
  has cache misses anyway.
\item Thread cache should be just big enough to reduce locking overhead.  About 10 objects.
\item Hardware transactional memory wins with simpler data structures.
\item Object sizes should be a prime number of cache lines to avoid associativity conflicts.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{x86-64 Address Space}

\input{x86-addresses_pdftex}
\end{frame}

\begin{frame}
\frametitle{Chunk Map}

Chunks are 2MiB (the medium page size on x86.)

To convert a pointer to a chunk number, divide by $2^{21}$.

Don't use a table, use an array.

There are only $2^{27}$ possible chunks.  Use 32 bits of information
per chunk, for $2^{29}$ bytes.

A program that uses $128$ GiB of allocated data needs only $65,536$
chunks.  Used half a gigabyte of address space, but only a quarter
megabyte of RSS.

It happens that \mintinline{c}{mmap()} returns data in a contiguous region.

Determining the size of an object takes 1 memory reference.

\end{frame}

\begin{frame}
\frametitle{Page Heap}

Insert cost is $O(1)$.  Remove cost is amortized $O(1)$, but worst
case $O(d)$ where $d$ is the number of items that fit into a page.

\end{frame}

\begin{frame}
\frametitle{Fullest-Page Heap}
\input{small-number-heap_pdftex}
\end{frame}

% What causes HTM to fail.

\begin{frame}
\frametitle{Performance Comparison}
\hspace*{-.7cm}\includegraphics{new-malloc-test-1K-aggregated.pdf}
\end{frame}

\end{document}

\begin{frame}[fragile]
\frametitle{An Implementation of Memalign}

Idea: allocate a little extra, and then return adjust the pointer to be aligned.

\begin{minted}[mathescape]{c}
void* memalign(size_t alignment, size_t s) {
  size_t p = (size_t)malloc(s+alignment-1);
  size_t a = (p+alignment-1)&~(alignment-1);
  return (void*)a;
}
\end{minted}

Bug: This implementation is wrong, because objects returned by
\mintinline{c}{memalign()} can be passed to \mintinline{c}{free()},
which requires that its argument is a block returned by
\mintinline{c}{malloc()}.


\end{frame}



the problem(s) with jemalloc
 visible problems:
   1) occasional 3-second call to free() (I don't know if I've fixed this, but it seems likely.  It's tough to reproduce)
   2) large memory footprint (essentially 2x?)
   3) Cache-index unfriendly
 mechanisms
   1) lowest-address allocation (an interesting heuristic.  May not be a problem itself. The data structure to calculate this may be a problem?)
   2) many arenas (the thread cache is too big?)

also want to investigate
 programming with transactional memory (this is weak, since I don't really see a performance advantage)
   1) What is TM
   2) How does it work (cached watching)
   3) Transactions fail--> need a fallback
     a) Subscribe to a lock (explain subscribe)
     b) Issues with late subscription/early subscription
   4) Why do transactions fail?
     a) interrupts (in particular, time slice interupt --> no transactions longer than 10ms)
     b) cache capacity (the read set can be in L2, the write set must fit in L1 - note this means that writes should be delayed if possible)
     c) actual conflicts
     d) Other random failures (the failure codes are essentially useless, except for the ``user aborted'' code)
   5) Tricks
     a) don't enter the transaction until the lock is available
     b) subscribe to the lock late
     c) prefetch data before entering the transaction (prefetch for write on writeable data) - this optimization doesn't seem to matter much)
     d) after doing all this, locks are just as fast as transactions


threaded random number generation with no initialization (Dice, https://blogs.oracle.com/dave/entry/a_simple_prng_idiom)
division by object sizes
batch move from thread cache to global cache
